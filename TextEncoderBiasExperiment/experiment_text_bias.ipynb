{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dab288ca-a97a-45d4-8e61-941a46fec47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1ooVVPxB-tvptgmHlIMMFGV3Cg-IrhbRZ\n",
      "From (redirected): https://drive.google.com/uc?id=1ooVVPxB-tvptgmHlIMMFGV3Cg-IrhbRZ&confirm=t&uuid=67b3c3b5-e9b2-4ff2-ba0a-70677f009208\n",
      "To: /home/user01/text_encoder_bias/negCLIP.pt\n",
      "100%|██████████████████████████████████████| 1.82G/1.82G [03:50<00:00, 7.88MB/s]\n"
     ]
    }
   ],
   "source": [
    "# ! gdown 1ooVVPxB-tvptgmHlIMMFGV3Cg-IrhbRZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e079ffc-8483-49e1-8a8b-2c39731b82a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\EA\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from transformers import AlignProcessor, AlignModel, AutoTokenizer\n",
    "from transformers import AltCLIPModel, AltCLIPProcessor\n",
    "from transformers import AutoProcessor, BlipModel\n",
    "from transformers import FlavaProcessor, FlavaForPreTraining, BertTokenizer, FlavaFeatureExtractor\n",
    "from transformers import ViltProcessor, ViltForImageAndTextRetrieval\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f68535f-8002-4102-b1ef-1c63eecd8dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db076d4-6d91-4198-aee7-91d492dd6667",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"open_clip\": [\n",
    "        # [\"ViT-B-32\", \"negCLIP.pt\"], first download and add negClip weights in this directory\n",
    "        [\"EVA01-g-14\", \"laion400m_s11b_b41k\"],\n",
    "        [\"EVA02-L-14\", \"merged2b_s4b_b131k\"],\n",
    "        [\"RN50x64\", \"openai\"],\n",
    "        [\"ViT-B-16\", \"openai\"],\n",
    "        [\"ViT-B-32\", \"openai\"],\n",
    "        [\"ViT-L-14\", \"openai\"],\n",
    "        [\"coca_ViT-B-32\", \"laion2b_s13b_b90k\"],\n",
    "        [\"coca_ViT-L-14\", \"laion2b_s13b_b90k\"],\n",
    "        ],\n",
    "    \"align\": [\"kakaobrain/align-base\"],\n",
    "    \"alt\": [\"BAAI/AltCLIP\"],\n",
    "    \"flava\": [\"facebook/flava-full\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c2dc12-32c8-4a0f-b2a9-5e6db8e7912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_clip\n",
    "def load_open_clip(activities , report_dict ):\n",
    "    for base_name, pretrained in MODELS[\"open_clip\"]:\n",
    "        print(base_name, pretrained)\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(base_name, pretrained=pretrained)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        tokenizer = open_clip.get_tokenizer(base_name)\n",
    "        for activity in activities:\n",
    "            tokenized_text = tokenizer(f\"A person is {preprocess_activity(activity)}\").to(device)\n",
    "            tokenized_male_text = tokenizer(f\"A man is {preprocess_activity(activity)}\").to(device)\n",
    "            tokenized_female_text = tokenizer(f\"A woman is {preprocess_activity(activity)}\").to(device)\n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(tokenized_text)\n",
    "                text_features_norm = text_features.norm(dim=-1)\n",
    "                male_text_features = model.encode_text(tokenized_male_text)\n",
    "                male_text_features_norm = male_text_features.norm(dim=-1)\n",
    "                female_text_features = model.encode_text(tokenized_female_text)\n",
    "                female_text_features_norm = female_text_features.norm(dim=-1)\n",
    "                male_sim = ((text_features @ male_text_features.T) / (text_features_norm * male_text_features_norm)).item()\n",
    "                female_sim = ((text_features @ female_text_features.T) / (text_features_norm * female_text_features_norm)).item()\n",
    "                sim_probs = torch.tensor([male_sim, female_sim]).softmax(dim=-1)\n",
    "                male_sim_prob, female_sim_prob = sim_probs[0].item(), sim_probs[1].item()\n",
    "            report_dict[\"model\"].append(f\"{base_name} {pretrained}\")\n",
    "            report_dict[\"activity\"].append(activity)\n",
    "            report_dict[\"male_sim_prob\"].append(np.round(male_sim_prob, 3))\n",
    "            report_dict[\"female_sim_prob\"].append(np.round(female_sim_prob, 3))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6372a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_activity(activity):\n",
    "    return activity.replace(\"_\", \" \").lower()\n",
    "\n",
    "def reverse_gender(gender):\n",
    "    return \"man\" if gender == \"woman\" else \"woman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f62611ea-6b60-44a2-9e6a-87137479f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# align\n",
    "def load_align(activities , report_dict):\n",
    "    for model_name in MODELS[\"align\"]:\n",
    "        print(model_name)\n",
    "        model = AlignModel.from_pretrained(model_name).to(device)\n",
    "        model.eval()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        for activity in activities:\n",
    "            tokenized_text = tokenizer(f\"A person is {preprocess_activity(activity)}\", padding=True, return_tensors=\"pt\").to(device) \n",
    "            tokenized_male_text = tokenizer(f\"A man is {preprocess_activity(activity)}\", padding=True, return_tensors=\"pt\").to(device)\n",
    "            tokenized_female_text = tokenizer(f\"A woman is {preprocess_activity(activity)}\", padding=True, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                text_features = model.get_text_features(**tokenized_text)\n",
    "                text_features_norm = text_features.norm(dim=-1)\n",
    "                male_text_features = model.get_text_features(**tokenized_male_text)\n",
    "                male_text_features_norm = male_text_features.norm(dim=-1)\n",
    "                female_text_features = model.get_text_features(**tokenized_female_text)\n",
    "                female_text_features_norm = female_text_features.norm(dim=-1)\n",
    "                male_sim = ((text_features @ male_text_features.T) / (text_features_norm * male_text_features_norm)).item()\n",
    "                female_sim = ((text_features @ female_text_features.T) / (text_features_norm * female_text_features_norm)).item()\n",
    "                sim_probs = torch.tensor([male_sim, female_sim]).softmax(dim=-1)\n",
    "                male_sim_prob, female_sim_prob = sim_probs[0].item(), sim_probs[1].item()\n",
    "            report_dict[\"model\"].append(f\"{model_name}\")\n",
    "            report_dict[\"activity\"].append(activity)\n",
    "            report_dict[\"male_sim_prob\"].append(np.round(male_sim_prob, 3))\n",
    "            report_dict[\"female_sim_prob\"].append(np.round(female_sim_prob, 3))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bfd3579-6b9a-41ba-8491-3d064e80a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt\n",
    "def load_alt(activities , report_dict):\n",
    "    for model_name in MODELS[\"alt\"]:\n",
    "        print(model_name)\n",
    "        model = AltCLIPModel.from_pretrained(model_name).to(device)\n",
    "        model.eval()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        for activity in activities:\n",
    "            tokenized_text = tokenizer(f\"A person is {preprocess_activity(activity)}\", padding=True, return_tensors=\"pt\").to(device) \n",
    "            tokenized_male_text = tokenizer(f\"A man is {preprocess_activity(activity)}\", padding=True, return_tensors=\"pt\").to(device)\n",
    "            tokenized_female_text = tokenizer(f\"A woman is {preprocess_activity(activity)}\", padding=True, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                text_features = model.get_text_features(**tokenized_text)\n",
    "                text_features_norm = text_features.norm(dim=-1)\n",
    "                male_text_features = model.get_text_features(**tokenized_male_text)\n",
    "                male_text_features_norm = male_text_features.norm(dim=-1)\n",
    "                female_text_features = model.get_text_features(**tokenized_female_text)\n",
    "                female_text_features_norm = female_text_features.norm(dim=-1)\n",
    "                male_sim = ((text_features @ male_text_features.T) / (text_features_norm * male_text_features_norm)).item()\n",
    "                female_sim = ((text_features @ female_text_features.T) / (text_features_norm * female_text_features_norm)).item()\n",
    "                sim_probs = torch.tensor([male_sim, female_sim]).softmax(dim=-1)\n",
    "                male_sim_prob, female_sim_prob = sim_probs[0].item(), sim_probs[1].item()\n",
    "            report_dict[\"model\"].append(f\"{model_name}\")\n",
    "            report_dict[\"activity\"].append(activity)\n",
    "            report_dict[\"male_sim_prob\"].append(np.round(male_sim_prob, 3))\n",
    "            report_dict[\"female_sim_prob\"].append(np.round(female_sim_prob, 3))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fba0368-6d5d-4c85-80e3-d73d0c7cff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flava\n",
    "def load_flava(activities , report_dict):\n",
    "    for model_name in MODELS[\"flava\"]:\n",
    "        print(model_name)\n",
    "        model = FlavaForPreTraining.from_pretrained(model_name).to(device)\n",
    "        model.eval()\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        for activity in activities:\n",
    "            tokenized_text = tokenizer(f\"A person is {preprocess_activity(activity)}\", padding=True, return_tensors=\"pt\").to(device) \n",
    "            tokenized_male_text = tokenizer(f\"A man is {preprocess_activity(activity)}\", padding=True, return_tensors=\"pt\").to(device)\n",
    "            tokenized_female_text = tokenizer(f\"A woman is {preprocess_activity(activity)}\", padding=True, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                text_features = model.flava.get_text_features(**tokenized_text)[:, 0, :]\n",
    "                text_features_norm = text_features.norm(dim=-1)\n",
    "                male_text_features = model.flava.get_text_features(**tokenized_male_text)[:, 0, :]\n",
    "                male_text_features_norm = male_text_features.norm(dim=-1)\n",
    "                female_text_features = model.flava.get_text_features(**tokenized_female_text)[:, 0, :]\n",
    "                female_text_features_norm = female_text_features.norm(dim=-1)\n",
    "                male_sim = ((text_features @ male_text_features.T) / (text_features_norm * male_text_features_norm)).item()\n",
    "                female_sim = ((text_features @ female_text_features.T) / (text_features_norm * female_text_features_norm)).item()\n",
    "                sim_probs = torch.tensor([male_sim, female_sim]).softmax(dim=-1)\n",
    "                male_sim_prob, female_sim_prob = sim_probs[0].item(), sim_probs[1].item()\n",
    "            report_dict[\"model\"].append(f\"{model_name}\")\n",
    "            report_dict[\"activity\"].append(activity)\n",
    "            report_dict[\"male_sim_prob\"].append(np.round(male_sim_prob, 3))\n",
    "            report_dict[\"female_sim_prob\"].append(np.round(female_sim_prob, 3))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c8fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for version in range (1 , 4):\n",
    "    DATASET_BASE_ADDRESS = f\"..\\\\phaze{version}\\\\Phaze-{version}\"\n",
    "    expected_genders_df = pd.read_csv(f\"{DATASET_BASE_ADDRESS}/expected_genders.csv\")\n",
    "    expected_genders = {activity: expected_genders_df.to_dict(\"list\")[\"gender\"][index]\n",
    "                        for index, activity in enumerate(expected_genders_df.to_dict(\"list\")[\"activity\"])}\n",
    "    activities = [activity for activity in os.listdir(f\"{DATASET_BASE_ADDRESS}/\") if activity[0] != \".\" and activity !='expected_genders.csv']\n",
    "\n",
    "    report_dict = {\"model\": [], \n",
    "               \"activity\":[], \n",
    "               \"male_sim_prob\": [], \n",
    "               \"female_sim_prob\": []}\n",
    "    \n",
    "    load_open_clip(activities , report_dict)\n",
    "    load_align(activities , report_dict)\n",
    "    load_alt(activities , report_dict)\n",
    "    load_flava(activities , report_dict )\n",
    "\n",
    "    RESULTS_BASE_ADDRESS = f\"Results/phaze{version}\"\n",
    "\n",
    "    if not os.path.exists(\"results\"):\n",
    "        os.makedirs(\"results\")\n",
    "\n",
    "    if not os.path.exists(RESULTS_BASE_ADDRESS):\n",
    "        os.makedirs(RESULTS_BASE_ADDRESS)\n",
    "\n",
    "    pd.DataFrame(data=report_dict).to_csv(f\"{RESULTS_BASE_ADDRESS}/text_encoder_bias.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d433ddf8-bbda-4793-a742-11185c9b5106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
