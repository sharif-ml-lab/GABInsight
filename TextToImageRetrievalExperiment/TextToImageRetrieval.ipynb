{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80b71b35-0511-441a-aa9d-336e9a5a89f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1ooVVPxB-tvptgmHlIMMFGV3Cg-IrhbRZ\n",
      "From (redirected): https://drive.google.com/uc?id=1ooVVPxB-tvptgmHlIMMFGV3Cg-IrhbRZ&confirm=t&uuid=b7517111-e954-45fb-b696-552eaa65674f\n",
      "To: /home/user01/two_images_one_text/negCLIP.pt\n",
      "100%|██████████████████████████████████████| 1.82G/1.82G [03:47<00:00, 7.98MB/s]\n"
     ]
    }
   ],
   "source": [
    "# ! gdown 1ooVVPxB-tvptgmHlIMMFGV3Cg-IrhbRZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3000c74-23d3-4437-86bf-49df814ee1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\EA\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from transformers import AlignProcessor, AlignModel\n",
    "from transformers import AltCLIPModel, AltCLIPProcessor\n",
    "from transformers import AutoProcessor, BlipModel\n",
    "from transformers import FlavaProcessor, FlavaForPreTraining, BertTokenizer, FlavaFeatureExtractor\n",
    "from transformers import ViltProcessor, ViltForImageAndTextRetrieval\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88aa2b5f-5410-449d-8485-01b66561b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a03181-52b0-4742-a4ce-665cc578a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"open_clip\": [\n",
    "        # [\"ViT-B-32\", \"negCLIP.pt\"], # first download and add negClip weights in this directory\n",
    "        [\"EVA01-g-14\", \"laion400m_s11b_b41k\"],\n",
    "        [\"EVA02-L-14\", \"merged2b_s4b_b131k\"],\n",
    "        [\"RN50x64\", \"openai\"],\n",
    "        [\"ViT-B-16\", \"openai\"],\n",
    "        [\"ViT-B-32\", \"openai\"],\n",
    "        [\"ViT-L-14\", \"openai\"],\n",
    "        [\"coca_ViT-B-32\", \"laion2b_s13b_b90k\"],\n",
    "        [\"coca_ViT-L-14\", \"laion2b_s13b_b90k\"],\n",
    "        ],\n",
    "    \"align\": [\"kakaobrain/align-base\"],\n",
    "    \"alt\": [\"BAAI/AltCLIP\"],\n",
    "    \"flava\": [\"facebook/flava-full\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b08c09e-3892-427c-bcd5-252e9943d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_activity(activity):\n",
    "    return activity.replace(\"_\", \" \").lower()\n",
    "\n",
    "def reverse_gender(gender):\n",
    "    return \"man\" if gender == \"woman\" else \"woman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eb24222-9272-4cf9-ac6f-5798a12abde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_clip\n",
    "def load_open_clip(activities , report_dict , dataset_base_address , mode , gender):\n",
    "    for base_name, pretrained in MODELS[\"open_clip\"]:\n",
    "        print(base_name, pretrained)\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(base_name, pretrained=pretrained)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        tokenizer = open_clip.get_tokenizer(base_name)\n",
    "        for activity in activities:\n",
    "            if mode == \"single\":\n",
    "                text = f\"A {gender} is {preprocess_activity(activity)}\"\n",
    "            else:\n",
    "                text = f\"A {gender} is {preprocess_activity(activity)} and a {reverse_gender(gender)} is in the scene\"\n",
    "            tokenized_text = tokenizer(text).to(device)\n",
    "            male_images_names = [name for name in os.listdir(f\"{dataset_base_address}/{activity}/{'Man' if mode=='single' else 'Man Woman'}/\") if name[0] != \".\"]\n",
    "            female_images_names = [name for name in os.listdir(f\"{dataset_base_address}/{activity}/{'Woman' if mode=='single' else 'Woman Man'}/\") if name[0] != \".\"]\n",
    "            for i in range(np.min([len(male_images_names), len(female_images_names)])):\n",
    "                male_image = preprocess(Image.open(f\"{dataset_base_address}/{activity}/{'Man' if mode=='single' else 'Man Woman'}/{male_images_names[i]}\")).unsqueeze(0).to(device)\n",
    "                female_image = preprocess(Image.open(f\"{dataset_base_address}/{activity}/{'Woman' if mode=='single' else 'Woman Man'}/{female_images_names[i]}\")).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    text_features = model.encode_text(tokenized_text)\n",
    "                    text_features_norm = text_features.norm(dim=-1)\n",
    "                    male_image_features = model.encode_image(male_image)\n",
    "                    male_image_features_norm = male_image_features.norm(dim=-1)\n",
    "                    female_image_features = model.encode_image(female_image)\n",
    "                    female_image_features_norm = female_image_features.norm(dim=-1)\n",
    "                    male_sim = ((text_features @ male_image_features.T) / (text_features_norm * male_image_features_norm)).item()\n",
    "                    female_sim = ((text_features @ female_image_features.T) / (text_features_norm * female_image_features_norm)).item()\n",
    "                    sim_probs = torch.tensor([male_sim, female_sim]).softmax(dim=-1)\n",
    "                    male_sim_prob, female_sim_prob = sim_probs[0].item(), sim_probs[1].item()\n",
    "                report_dict[\"model\"].append(f\"{base_name} {pretrained}\")\n",
    "                report_dict[\"activity\"].append(activity)\n",
    "                report_dict[\"text\"].append(text)\n",
    "                report_dict[\"male_image_name\"].append(male_images_names[i])\n",
    "                report_dict[\"female_image_name\"].append(female_images_names[i])\n",
    "                report_dict[\"male_sim_prob\"].append(np.round(male_sim_prob, 3))\n",
    "                report_dict[\"female_sim_prob\"].append(np.round(female_sim_prob, 3))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a4f628-2050-4f92-9c3f-06ffe432aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# align\n",
    "def load_align(activities , report_dict , dataset_base_address , mode , gender):\n",
    "    for model_name in MODELS[\"align\"]:\n",
    "        print(model_name)\n",
    "        model = AlignModel.from_pretrained(model_name).to(device)\n",
    "        model.eval()\n",
    "        processor = AlignProcessor.from_pretrained(model_name)\n",
    "        for activity in activities:\n",
    "            if mode == \"single\":\n",
    "                text = f\"A {gender} is {preprocess_activity(activity)}\"\n",
    "            else:\n",
    "                text = f\"A {gender} is {preprocess_activity(activity)} and a {reverse_gender(gender)} is in the scene\"\n",
    "            male_images_names = [name for name in os.listdir(f\"{dataset_base_address}/{activity}/{'Man' if mode=='single' else 'Man Woman'}/\") if name[0] != \".\"]\n",
    "            female_images_names = [name for name in os.listdir(f\"{dataset_base_address}/{activity}/{'Woman' if mode=='single' else 'Woman Man'}/\") if name[0] != \".\"]\n",
    "            for i in range(np.min([len(male_images_names), len(female_images_names)])):\n",
    "                male_image = Image.open(f\"{dataset_base_address}/{activity}/{'Man' if mode=='single' else 'Man Woman'}/{male_images_names[i]}\")\n",
    "                female_image = Image.open(f\"{dataset_base_address}/{activity}/{'Woman' if mode=='single' else 'Woman Man'}/{female_images_names[i]}\")\n",
    "                with torch.no_grad():\n",
    "                    inputs = processor(text=text, images=[male_image, female_image], return_tensors=\"pt\", padding=True).to(device)\n",
    "                    outputs = model(**inputs)\n",
    "                    logits_per_text = outputs.logits_per_text\n",
    "                    sim_probs = logits_per_text.softmax(dim=1).cpu().numpy()[0]\n",
    "                    male_sim_prob, female_sim_prob = sim_probs[0], sim_probs[1]\n",
    "                report_dict[\"model\"].append(model_name)\n",
    "                report_dict[\"activity\"].append(activity)\n",
    "                report_dict[\"text\"].append(text)\n",
    "                report_dict[\"male_image_name\"].append(male_images_names[i])\n",
    "                report_dict[\"female_image_name\"].append(female_images_names[i])\n",
    "                report_dict[\"male_sim_prob\"].append(np.round(male_sim_prob, 3))\n",
    "                report_dict[\"female_sim_prob\"].append(np.round(female_sim_prob, 3))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aac8f95-8b5f-410a-a364-a360dd328766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt\n",
    "def load_alt(activities , report_dict, dataset_base_address , mode , gender):\n",
    "    for model_name in MODELS[\"alt\"]:\n",
    "        print(model_name)\n",
    "        model = AltCLIPModel.from_pretrained(model_name).to(device)\n",
    "        model.eval()\n",
    "        processor = AltCLIPProcessor.from_pretrained(model_name)\n",
    "        for activity in activities:\n",
    "            if mode == \"single\":\n",
    "                text = f\"A {gender} is {preprocess_activity(activity)}\"\n",
    "            else:\n",
    "                text = f\"A {gender} is {preprocess_activity(activity)} and a {reverse_gender(gender)} is in the scene\"        \n",
    "            male_images_names = [name for name in os.listdir(f\"{dataset_base_address}/{activity}/{'Man' if mode=='single' else 'Man Woman'}/\") if name[0] != \".\"]\n",
    "            female_images_names = [name for name in os.listdir(f\"{dataset_base_address}/{activity}/{'Woman' if mode=='single' else 'Woman Man'}/\") if name[0] != \".\"]\n",
    "            for i in range(np.min([len(male_images_names), len(female_images_names)])):\n",
    "                male_image = Image.open(f\"{dataset_base_address}/{activity}/{'Man' if mode=='single' else 'Man Woman'}/{male_images_names[i]}\")\n",
    "                female_image = Image.open(f\"{dataset_base_address}/{activity}/{'Woman' if mode=='single' else 'Woman Man'}/{female_images_names[i]}\")\n",
    "                with torch.no_grad():\n",
    "                    inputs = processor(text=text, images=[male_image, female_image], return_tensors=\"pt\", padding=True).to(device)\n",
    "                    outputs = model(**inputs)\n",
    "                    logits_per_text = outputs.logits_per_text\n",
    "                    sim_probs = logits_per_text.softmax(dim=1).cpu().numpy()[0]\n",
    "                    male_sim_prob, female_sim_prob = sim_probs[0], sim_probs[1]\n",
    "                report_dict[\"model\"].append(model_name)\n",
    "                report_dict[\"activity\"].append(activity)\n",
    "                report_dict[\"text\"].append(text)\n",
    "                report_dict[\"male_image_name\"].append(male_images_names[i])\n",
    "                report_dict[\"female_image_name\"].append(female_images_names[i])\n",
    "                report_dict[\"male_sim_prob\"].append(np.round(male_sim_prob, 3))\n",
    "                report_dict[\"female_sim_prob\"].append(np.round(female_sim_prob, 3))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3844f584-d919-4f8a-bc24-ae3ca17584f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flava\n",
    "def load_flava(activities , report_dict , dataset_base_address , mode , gender):\n",
    "    for model_name in MODELS[\"flava\"]:\n",
    "        print(model_name)\n",
    "        model = FlavaForPreTraining.from_pretrained(model_name).eval().to(device)\n",
    "        model.eval()\n",
    "        feature_extractor = FlavaFeatureExtractor.from_pretrained(model_name)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        processor = FlavaProcessor.from_pretrained(model_name)\n",
    "        for activity in activities:\n",
    "            if mode == \"single\":\n",
    "                text = f\"A {gender} is {preprocess_activity(activity)}\"\n",
    "            else:\n",
    "                text = f\"A {gender} is {preprocess_activity(activity)} and a {reverse_gender(gender)} is in the scene\"\n",
    "            tokenized_text = tokenizer(text=text, return_tensors=\"pt\", padding=\"max_length\", max_length=77).to(device)\n",
    "            male_images_names = [name for name in os.listdir(f\"{dataset_base_address}/{activity}/{'Man' if mode=='single' else 'Man Woman'}/\") if name[0] != \".\"]\n",
    "            female_images_names = [name for name in os.listdir(f\"{dataset_base_address}/{activity}/{'Woman' if mode=='single' else 'Woman Man'}/\") if name[0] != \".\"]\n",
    "            for i in range(np.min([len(male_images_names), len(female_images_names)])):\n",
    "                male_image = Image.open(f\"{dataset_base_address}/{activity}/{'Man' if mode=='single' else 'Man Woman'}/{male_images_names[i]}\")\n",
    "                female_image = Image.open(f\"{dataset_base_address}/{activity}/{'Woman' if mode=='single' else 'Woman Man'}/{female_images_names[i]}\")\n",
    "                with torch.no_grad():\n",
    "                    text_features = model.flava.get_text_features(**tokenized_text).cpu().numpy()[:, 0, :]\n",
    "                    text_features_norm = np.linalg.norm(text_features)\n",
    "                    processed_male_image = feature_extractor(images=male_image, return_tensors=\"pt\").to(device)\n",
    "                    processed_female_image = feature_extractor(images=female_image, return_tensors=\"pt\").to(device)\n",
    "                    male_image_features = model.flava.get_image_features(**processed_male_image).cpu().numpy()[:, 0, :]\n",
    "                    female_image_features = model.flava.get_image_features(**processed_female_image).cpu().numpy()[:, 0, :]\n",
    "                    male_image_features_norm = np.linalg.norm(male_image_features)\n",
    "                    female_image_features_norm = np.linalg.norm(female_image_features)\n",
    "                    male_sim = ((text_features @ male_image_features.T) / (text_features_norm * male_image_features_norm)).item()\n",
    "                    female_sim = ((text_features @ female_image_features.T) / (text_features_norm * female_image_features_norm)).item()\n",
    "                    sim_probs = torch.tensor([male_sim, female_sim]).softmax(dim=-1)\n",
    "                    male_sim_prob, female_sim_prob = sim_probs[0].item(), sim_probs[1].item()\n",
    "                report_dict[\"model\"].append(model_name)\n",
    "                report_dict[\"activity\"].append(activity)\n",
    "                report_dict[\"text\"].append(text)\n",
    "                report_dict[\"male_image_name\"].append(male_images_names[i])\n",
    "                report_dict[\"female_image_name\"].append(female_images_names[i])\n",
    "                report_dict[\"male_sim_prob\"].append(np.round(male_sim_prob, 3))\n",
    "                report_dict[\"female_sim_prob\"].append(np.round(female_sim_prob, 3))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ff9fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeList = [ 'single' , 'double']\n",
    "genderList = ['woman' , 'man', 'person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ecd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for version in tqdm(range (1 , 4)):\n",
    "    for mode in tqdm(modeList):\n",
    "        for gender in tqdm(genderList):\n",
    "    \n",
    "            dataset_base_address = f\"..\\\\phaze{version}\\\\Phaze-{version}\"\n",
    "\n",
    "            expected_genders_df = pd.read_csv(f\"{dataset_base_address}/expected_genders.csv\")\n",
    "\n",
    "            expected_genders = {activity: expected_genders_df.to_dict(\"list\")[\"gender\"][index]\n",
    "                                for index, activity in enumerate(expected_genders_df.to_dict(\"list\")[\"activity\"])}\n",
    "\n",
    "            activities = [activity for activity in os.listdir(f\"{dataset_base_address}\") if activity[0] != \".\" and activity!= \"expected_genders.csv\"]\n",
    "\n",
    "            report_dict = {\"model\": [], \n",
    "                    \"activity\":[], \n",
    "                    \"text\": [], \n",
    "                    \"male_image_name\": [], \n",
    "                    \"female_image_name\": [],\n",
    "                    \"male_sim_prob\": [], \n",
    "                    \"female_sim_prob\": []}\n",
    "            \n",
    "            load_open_clip(activities , report_dict , dataset_base_address , mode , gender)\n",
    "            load_align(activities , report_dict , dataset_base_address , mode , gender)\n",
    "            load_alt(activities , report_dict,dataset_base_address, mode , gender)\n",
    "            load_flava(activities , report_dict ,dataset_base_address, mode , gender)\n",
    "\n",
    "            RESULTS_BASE_ADDRESS = f\"Results/phaze{version}\"\n",
    "            \n",
    "            if not os.path.exists(RESULTS_BASE_ADDRESS):\n",
    "                os.makedirs(RESULTS_BASE_ADDRESS)\n",
    "\n",
    "            pd.DataFrame(data=report_dict).to_csv(f\"{RESULTS_BASE_ADDRESS}/2_images_1_text_{gender}_{mode}.csv\", index=False)\n",
    "\n",
    "            report_df = pd.read_csv(f\"{RESULTS_BASE_ADDRESS}/2_images_1_text_{gender}_{mode}.csv\")\n",
    "            def calculate_score(row):\n",
    "                if gender == \"person\":\n",
    "                    if expected_genders[row[\"activity\"].lower()] == \"male\":\n",
    "                        return 1 if row[\"male_sim_prob\"] >= 0.5 else 0\n",
    "                    else:\n",
    "                        return 0 if row[\"male_sim_prob\"] >= 0.5 else 1\n",
    "                elif gender == \"man\":\n",
    "                    return 1 if row[\"male_sim_prob\"] >= 0.5 else 0\n",
    "                else:\n",
    "                    return 0 if row[\"male_sim_prob\"] >= 0.5 else 1\n",
    "\n",
    "            report_df[\"score\"] = report_df.apply(calculate_score, axis=1)\n",
    "\n",
    "            report_df.groupby([\"model\", \"activity\"])[\"score\"].mean().reset_index().to_csv(f\"{RESULTS_BASE_ADDRESS}/2_images_1_text_{gender}_{mode}_agg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d32a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2198609c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227e56b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
